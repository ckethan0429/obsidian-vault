---
title: "가중치 업데이트 & 경사 하강법(Gradient Descent) 정리"
date: 2026-02-19
tags:
  - deep-learning
  - optimization
  - gradient-descent
  - backpropagation
  - study-notes
---

# 가중치 업데이트 & 경사 하강법(Gradient Descent) 정리

## 1) 문제의 출발점: 가중치를 어떻게 업데이트할 것인가?
신경망 학습의 목표는 **오차(Error, Loss)** 를 줄이는 것입니다.

하지만 현실적으로는:
- 가중치가 매우 많습니다 (수백 ~ 수억 개)
- 출력은 “함수의 함수의 함수…” 구조(층이 쌓인 합성함수)
- 수식이 너무 복잡해 **최적 가중치를 한 번에(폐형식으로) 계산**하기 어렵습니다

즉, 대수학적으로 한 번에 해를 구하는 방식은 **현실적으로 불가능**에 가깝습니다.

---

## 2) 무차별 대입(Brute Force)은 왜 불가능한가?
가중치 수가 많아지면 조합이 **기하급수적으로 폭발**합니다.

예시:
- 가중치 50만 개
- 각 가중치가 1000가지 값을 가질 수 있다고 가정

가능한 조합 수는 `1000^(500,000)` 수준으로, 계산/실험으로 전부 확인하는 것은 시간적으로 불가능합니다.

결론: **Brute Force는 실용적이지 않습니다.**

---

## 3) 해결책: 경사 하강법(Gradient Descent)

### 핵심 아이디어
“완벽한 해를 한 번에 구하려 하지 말고, **조금씩 개선**하자.”

### 직관적 비유: 어두운 산에서 내려오기
- 지도 없음
- 손전등 하나
- 목표: 가장 낮은 곳(오차 최소)으로 내려가기

행동:
1. 주변(현재 위치 근처)을 살핀다
2. 더 낮아지는 방향을 찾는다
3. 그 방향으로 한 걸음 이동한다
4. 반복한다

이 반복 과정이 경사 하강법의 직관입니다.

---

## 4) 수학적으로는 무엇을 하는가?
우리는 **오차 함수(loss)의 기울기(gradient)** 를 계산합니다.

기울기는 다음을 알려줍니다.
- 어느 방향으로 가야 오차가 줄어드는지
- 얼마나 크게 이동해야 하는지(학습률과 결합)

### 기본 업데이트 공식
가중치(파라미터) \(w\)에 대해:

$$
w_{new} = w - \eta \cdot \nabla_w L
$$

- \(\nabla_w L\): 오차 \(L\)을 \(w\)로 미분한 기울기(오차를 가장 빠르게 **증가**시키는 방향)
  - 따라서 **마이너스(-)** 를 붙여 오차가 줄어드는 방향으로 이동
- \(\eta\) (eta): 학습률(learning rate), 한 번에 얼마나 이동할지

---

## 5) 매개변수가 많아질 때(고차원 오차 지형)

### 단순한 경우(변수 1개)
예: \(y = (x - 1)^2 + 1\)
- 변수 1개
- 그래프는 단순한 U자 형태

### 신경망의 경우(변수 수백~수억 개)
예: \(L = f(a, b, c, d, e, ...)\)
- 고차원(매우 많은 축)의 표면
- 오차 지형이 복잡하고, 여러 골짜기/봉우리/평지(plateau)가 존재

---

## 6) Local Minimum(국소 최소) 문제
복잡한 함수에는 계곡이 여러 개 존재할 수 있습니다.

용어 정리:
- **Global Minimum**: 전체에서 가장 낮은 지점
- **Local Minimum**: 주변에서는 가장 낮지만, 전체 최저점은 아님

경사 하강법은 **출발점(초기값)** 에 따라 local minimum에 빠질 수 있습니다.

---

## 7) (기초적) 해결 방법
- 가중치 초기값을 다르게 해서 여러 번 학습
  - 랜덤 초기화
  - 여러 번 훈련
  - 가장 좋은 결과 선택

이 방식은 지금도 널리 쓰입니다.

---

## 8) 경사 하강법의 장점
- 복잡한 함수에서도 작동
- 매개변수가 많아도 작동
- 데이터가 완벽하지 않아도 작동
- “수학적으로 완벽한 해”를 몰라도 최적화 가능

---

## 9) 한 줄 요약
신경망 학습이란, **오차라는 산을 조금씩 내려오는 과정**입니다.

---

## 다음 개념(자연스럽게 이어지는 주제)
- **역전파(Backpropagation)**: 기울기 \(\nabla L\)를 효율적으로 계산하는 방법
- **확률적 경사 하강법(SGD)**: 전체 데이터 대신 미니배치로 업데이트
- **학습률의 역할**: 너무 크면 발산, 너무 작으면 느림
- **Adam 등 고급 최적화 기법**: 모멘텀/적응적 학습률 등
