---
title: "오차함수(Loss) → 미분(Gradient) → 역전파(Backprop) → 가중치 업데이트: 전체 흐름"
date: 2026-02-19
tags:
  - deep-learning
  - loss-function
  - gradient
  - backpropagation
  - optimization
  - study-notes
---

# 오차함수(Loss) → 미분(Gradient) → 역전파(Backprop) → 가중치 업데이트: 전체 흐름

## 1) 목표는 하나
신경망 학습의 목표는 단순합니다.

- **가중치(Weight)를 바꿔서 오차(Error/Loss)를 줄인다.**

그런데 “오차를 줄인다”를 하려면 먼저 두 가지가 필요합니다.
1) 오차를 어떻게 정의할지(Loss Function)
2) 가중치를 조금 바꾸면 오차가 얼마나 변하는지(Gradient)

---

## 2) 오차함수(Loss Function): 출력 자체가 오차는 아니다
출력 \(o\) 자체는 오차가 아닙니다.
오차는 목표값 \(t\)와 실제 출력 \(o\)의 차이를 기반으로 정의합니다.

- 기본 차이: \(t - o\)

하지만 출력 노드가 여러 개면, 이를 **하나의 숫자**(스칼라)로 요약해야 최적화가 가능합니다.

### ❌ 후보 1: 그냥 차이의 합
\[
E = \sum (t - o)
\]
문제:
- 어떤 노드는 +오차, 어떤 노드는 -오차가 되어 **상쇄**될 수 있음
- 실제로는 틀렸는데도 \(E\)가 0처럼 보일 수 있음

### ⚠ 후보 2: 절댓값 오차
\[
E = \sum |t - o|
\]
장점:
- 상쇄 문제 해결

단점:
- 꼭짓점에서 기울기가 매끄럽지 않음(미분/최적화가 불안정해질 수 있음)

### ✅ 후보 3: 제곱오차(MSE 계열)
\[
E = \sum (t - o)^2
\]
왜 많이 쓰나?
- 음/양 상쇄 없음
- 곡선이 부드러움
- 미분이 쉬움
- 최저점에 가까워질수록 기울기가 자연스럽게 작아짐

---

## 3) 이제 핵심: 미분(Gradient)
경사 하강법을 하려면 알고 싶은 건 딱 하나입니다.

> 가중치 \(w\)를 조금 바꾸면 오차 \(E\)가 얼마나 변하는가?

즉,
\[
\frac{\partial E}{\partial w}
\]

이 값이 알려주는 것:
- 어느 방향으로 가야 \(E\)가 줄어드는지
- 얼마나 크게 움직여야 하는지(학습률과 결합)

---

## 4) 출력층 가중치의 기울기 구조(직관)
출력층 가중치 \(w_{jk}\) (은닉 j → 출력 k)에 대해, 흔히 다음 형태가 나옵니다(시그모이드 예시):

\[
\frac{\partial E}{\partial w_{jk}} = -(t_k - o_k) \cdot o_k(1 - o_k) \cdot o_j
\]

각 항의 의미:
1) \((t_k - o_k)\)
- 출력 노드의 오차(얼마나 틀렸는가)

2) \(o_k(1 - o_k)\)
- 시그모이드의 미분(민감도)
- 출력이 변화에 얼마나 민감한 구간인지

3) \(o_j\)
- 이전 층(은닉층) 노드의 출력(얼마나 강하게 들어왔는가)

### 한 줄 요약(매우 중요)
**가중치 변화량은**

> (출력 오차) × (활성화 함수의 민감도) × (이전 노드의 출력)

오차가 크면 많이 바뀌고,
이전 노드가 강하게 활성화되면 많이 바뀌고,
활성화 함수가 민감한 구간이면 많이 바뀝니다.

---

## 5) 은닉층 가중치: 목표값이 없어서 ‘오차를 뒤로 전달’한다
은닉층은 직접 목표값 \(t\)가 없습니다.
그래서 은닉층의 가중치는 **출력층의 오차를 뒤로 전달받아** “책임”을 계산합니다.

이 과정이 바로 **역전파(Backpropagation)** 입니다.

- 출력에서 생긴 오차 신호를 뒤로 흘려보내며
- 각 연결(가중치)이 오차에 기여한 정도(책임)를 계산

---

## 6) 최종 업데이트 공식(경사 하강)
가중치 업데이트는 다음 형태로 요약됩니다.

\[
w_{new} = w_{old} - \alpha \frac{\partial E}{\partial w}
\]

- \(\alpha\): 학습률(learning rate)
- **빼는 이유**: 오차를 줄이는 방향(gradient의 반대 방향)으로 이동

---

## 7) 행렬 관점: 사실상 ‘두 신호의 곱’이다
가중치 업데이트는 직관적으로 이렇게도 볼 수 있습니다.

- **가중치 변화 = (다음 층의 오차 신호 벡터) × (이전 층의 출력 벡터)**

즉, “다음 층 신호” × “이전 층 신호” 구조가 반복됩니다.
그래서 딥러닝 구현이 대부분 **행렬곱 중심**으로 구성됩니다.

---

## 8) 전체 흐름(한 번에 묶기)
1. 오차를 정의한다 (보통 제곱오차 같은 형태)
2. 오차를 가중치로 미분한다
3. 연쇄법칙으로 계산을 쪼갠다
4. (출력 오차) × (활성화 미분) × (이전 출력) 형태가 나온다
5. 학습률을 곱해 가중치를 업데이트한다
6. 반복한다

---

## 🎯 핵심 한 문장
신경망 학습은 **오차를 각 연결에 ‘책임 분배’** 해서 조금씩 줄여가는 과정입니다.

## 다음으로 이어질 개념
- Backprop의 일반화(다층/다중 출력)
- SGD/미니배치 학습
- 학습률 스케줄링
- Adam 등 최적화 알고리즘
